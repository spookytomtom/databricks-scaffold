{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fe3daa-e527-46b7-8340-933f7eec8180",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7cb37c7-3c63-46a4-bb8d-69dca65b252d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "repo_root = Path(os.getcwd()).parent\n",
    "src_path = str(repo_root / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from databricks_scaffold import VolumeSpiller, frame_shape\n",
    "\n",
    "# Configuration\n",
    "IS_DEV = True # In Dev, we preserve volumes; in Prod, we wipe them\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"default\"\n",
    "VOL_NAME = f\"showcase_vol_{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"VolumeSpillerShowcase\").getOrCreate()\n",
    "\n",
    "spill = VolumeSpiller(spark, CATALOG, SCHEMA, VOL_NAME, is_dev=IS_DEV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a0b5ae-8f57-407c-9d0a-f11381866ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e87b1f08-bcf5-495e-99be-245d48783155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(i, f\"User_{i%5}\", \"Electronics\" if i%2==0 else \"Books\", i * 15.5, \n",
    "         datetime(2024, 1, 1) + timedelta(days=i)) for i in range(1, 101)]\n",
    "\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"id\", T.IntegerType(), False),\n",
    "    T.StructField(\"user\", T.StringType(), True),\n",
    "    T.StructField(\"category\", T.StringType(), True),\n",
    "    T.StructField(\"amount\", T.DoubleType(), True),\n",
    "    T.StructField(\"timestamp\", T.TimestampType(), True)\n",
    "])\n",
    "\n",
    "df_spark = spark.createDataFrame(data, schema)\n",
    "print(f\"üìä Initial Spark DataFrame Count: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0162b059-7b69-4619-b264-5cad040b8f85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22f0b919-8d1b-4ba7-9fc2-86f65d0e99be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Persistent Spark Checkpoints\n",
    "\n",
    "Use this when you want to save intermediate Spark results without creating a formal managed table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee77af6e-21c2-4ed9-81ed-1069ace82297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spill.save_checkpoint_spark(df_spark, \"raw_transactions_spark\")\n",
    "\n",
    "# Reloading to verify\n",
    "df_reloaded = spill.load_checkpoint_spark(\"raw_transactions_spark\")\n",
    "print(f\"‚úÖ Reloaded {df_reloaded.count()} rows from Spark checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f3fb37-df9c-480d-a49b-55721cd248c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. The Conversion Bridge (Spark ‚û°Ô∏è Polars)\n",
    "\n",
    "When the data is small enough for a single node, flip to Polars for high-performance local processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027ad2f8-5fc3-4731-9aba-0bf29e47d9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert and immediately prep for Polars (fixing precision)\n",
    "df_polars = (\n",
    "    spill.spark_to_polars(df_spark, cleanup=True)\n",
    "    .with_columns(pl.col(\"timestamp\").dt.cast_time_unit(\"ms\"))\n",
    ")\n",
    "\n",
    "print(f\"‚ö° Data converted to Polars. Shape: {df_polars.shape}\")\n",
    "print(df_polars.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "745e2a1f-c476-40c9-98e7-b5ad43406ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Hybrid Storage Options\n",
    "\n",
    "VolumeSpiller supports two tiers of storage for Polars:\n",
    "\n",
    " - Volume: Persistent, accessible by other users/jobs.\n",
    " - Local: Ephemeral, fast, sits on the driver's /tmp directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e71294-c323-4cb6-be81-2e9bd21a968d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save to both tiers\n",
    "spill.save_checkpoint_pl(df_polars, \"processed_gold_vol\", storage=\"volume\")\n",
    "spill.save_checkpoint_pl(df_polars, \"temp_scratchpad\", storage=\"local\")\n",
    "\n",
    "print(\"üìÇ Current Checkpoints:\")\n",
    "print(f\"  Volume: {spill.list_checkpoints('volume')}\")\n",
    "print(f\"  Local:  {spill.list_checkpoints('local')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db3203c2-695f-4395-aa42-f1706e10c4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Closing the Loop (Polars ‚û°Ô∏è Spark)\n",
    "\n",
    "After doing your complex local logic in Polars, move the result back to Spark to join with massive tables or write to a final Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12878c65-ab1a-4d66-8ae0-23e93a78bcd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert back to Spark\n",
    "df_final_spark = spill.polars_to_spark(df_polars, cleanup=True)\n",
    "\n",
    "df_final_spark.select(\"user\", \"amount\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da772f06-83d9-4397-9078-bb412e089386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Extra: Datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a67caa-c24e-4dcd-8e63-7d0cfa07d8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create Polars DF with nanosecond precision (the default in many Polars operations)\n",
    "df_ns = pl.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"event_time\": [datetime(2024, 1, 1), datetime(2024, 1, 2), datetime(2024, 1, 3)]\n",
    "}).with_columns(pl.col(\"event_time\").dt.cast_time_unit(\"ns\"))\n",
    "\n",
    "print(f\"Original Precision: {df_ns.schema['event_time']}\") \n",
    "# Output: Datetime(time_unit='ns', time_zone=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17faf44-ae20-4f2c-b142-4d0a4d001274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Watch how VolumeSpiller detects the datetime precision issue and fixes it on the fly during conversion or saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70219e1b-026d-4022-8162-892b7734baee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_spark_time = spill.polars_to_spark(df_ns, cleanup=True)\n",
    "\n",
    "df_spark_time.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e1b73e8-c07a-4e96-a590-acc7bbfb219a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Cleanup & Safety\n",
    "\n",
    "The teardown() method behaves differently based on your IS_DEV flag.\n",
    "\n",
    " - In Dev: It leaves the Volume intact so you can go to the Catalog Explorer and inspect the Parquet files manually.\n",
    " - In Prod: It cleans up after itself to avoid storage costs and clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189d7d52-51cd-4677-8a07-695a054f82ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the environment before we finish\n",
    "print(f\"Cleanup mode: {'Preserving data' if IS_DEV else 'Dropping Volume'}\")\n",
    "spill.teardown()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "polars==1.38.1"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "exampleShowcase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
