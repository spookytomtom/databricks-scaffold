{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80f5cf9-3737-48a3-a80f-10aac073d0f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸš€ databricks-scaffold: Complete Showcase\n",
    "\n",
    "Productivity booster functions for using Polars on Databricks. Ease the pain of working with small data on Databricks. \n",
    "\n",
    "This notebook demonstrates the complete suite of features:\n",
    "1. **PySpark Utilities**: Missing quality-of-life functions for PySpark DataFrames (`clean_column_names`, `keep_duplicates`, `frame_shape`, `apply_column_comments`).\n",
    "2. **The \"Polars Sandwich\"**: Fast, zero-memory-spike conversions between distributed PySpark DataFrames and local Polars DataFrames using Unity Catalog Volumes.\n",
    "3. **Checkpointing**: Easy saving and loading to persistent Unity Catalog ðŸ›œvolume or âš¡ephemeral local driver storage.\n",
    "4. **Timestamp Fixes**: Silent, automatic handling of nanosecond to millisecond conversions to prevent Spark crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d282cd42-846c-4f13-8add-f7a939195e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "repo_root = Path(os.getcwd()).parent\n",
    "src_path = str(repo_root / \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import databricks-scaffold core and utilities\n",
    "from databricks_scaffold import (\n",
    "    VolumeSpiller, \n",
    "    frame_shape, \n",
    "    clean_column_names, \n",
    "    keep_duplicates, \n",
    "    apply_column_comments\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "IS_DEV = True # In Dev, we preserve volumes; in Prod, we wipe them\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"default\"\n",
    "VOL_NAME = f\"showcase_vol_{uuid.uuid4().hex[:6]}\"\n",
    "\n",
    "# Initialize Spark and VolumeSpiller\n",
    "spark = SparkSession.builder.appName(\"ScaffoldShowcase\").getOrCreate()\n",
    "spill = VolumeSpiller(spark, CATALOG, SCHEMA, VOL_NAME, is_dev=IS_DEV)\n",
    "\n",
    "print(f\"âœ… Initialization complete. Using volume: {spill.full_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb38470e-f663-44bf-bffb-8e09a9a33c3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ› ï¸ Part 1: PySpark Utility Functions\n",
    "First, let's create some messy PySpark data and clean it up using the scaffold's utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab5d580a-efe2-41d8-8b6a-7ad66fa16012",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate data with messy column names and intentional duplicates\n",
    "data = [\n",
    "    (1, \"Alice\", \"alice@test.com\", datetime(2024, 1, 1)),\n",
    "    (2, \"Bob\",   \"bob@test.com\",   datetime(2024, 1, 2)),\n",
    "    (1, \"Alice\", \"alice@test.com\", datetime(2024, 1, 1)), # Duplicate!\n",
    "    (3, \"Carol\", \"carol@test.com\", datetime(2024, 1, 3))\n",
    "]\n",
    "\n",
    "# Note the spaces and special characters in column names\n",
    "df_messy = spark.createDataFrame(data, [\"User ID #\", \"First Name!\", \"Email Address\", \"Created At\"])\n",
    "\n",
    "print(\"--- Original Messy DataFrame ---\")\n",
    "df_messy.show()\n",
    "\n",
    "# 1. clean_column_names()\n",
    "df_clean = clean_column_names(df_messy)\n",
    "print(\"--- After clean_column_names() ---\")\n",
    "print(df_clean.columns)\n",
    "\n",
    "# 2. keep_duplicates()\n",
    "df_dupes = keep_duplicates(df_clean, subset=[\"User_ID\", \"First_Name\"])\n",
    "print(\"--- Duplicated Rows Found ---\")\n",
    "df_dupes.show()\n",
    "\n",
    "# 3. frame_shape()\n",
    "print(\"--- DataFrame Shape ---\")\n",
    "shape = frame_shape(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb3e989-4e74-4006-94bd-855b13192dc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. apply_column_comments()\n",
    "# Let's save the table to Unity Catalog to apply comments to it\n",
    "table_name = f\"{CATALOG}.{SCHEMA}.scaffold_test_table\"\n",
    "\n",
    "(df_clean.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(table_name))\n",
    "\n",
    "comments = {\n",
    "    \"User_ID\": \"Unique identifier for the user.\",\n",
    "    \"First_Name\": \"User's given name.\",\n",
    "    \"Email_Address\": \"Contact email.\",\n",
    "    \"Created_At\": \"Account creation timestamp.\"\n",
    "}\n",
    "\n",
    "print(\"--- Applying Column Comments ---\")\n",
    "apply_column_comments(spark, table_name, comments=comments, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b09072e3-ab7d-463e-b365-4d4cd340c5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ¥ª Part 2: The \"Polars Sandwich\" (Spark âž¡ï¸ Polars âž¡ï¸ Spark)\n",
    "When the data is small enough for a single node, flip it to Polars for high-performance local processing, then seamlessly send it back to Spark. `VolumeSpiller` handles the I/O automatically via Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56df8291-aff4-4721-8118-5753d012a754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Spark to Polars (Converts and loads into Driver memory)\n",
    "# Using cleanup=False keeps the temporary files in volume after reading\n",
    "# Using eager=True reads as Dataframe, =False scans as Lazyframe\n",
    "pl_df = spill.spark_to_polars(df_clean, cleanup=False, eager=True)\n",
    "\n",
    "print(f\"âš¡ Converted to Polars. Shape: {pl_df.shape}\")\n",
    "\n",
    "# Step 2: Do heavy lifting in Polars \n",
    "# Let's drop the duplicates we found earlier and add a new column\n",
    "pl_processed = (\n",
    "    pl_df\n",
    "    .unique(subset=[\"User_ID\"])\n",
    "    .with_columns(\n",
    "        pl.col(\"First_Name\").str.to_uppercase().alias(\"First_Name_Upper\")\n",
    "    )\n",
    ")\n",
    "print(pl_processed.head())\n",
    "\n",
    "# Step 3: Polars to Spark\n",
    "# Automatically fixes datetime precision (ns/us -> ms) and attaches UTC timezone\n",
    "# Polars defaults to ns/us, can cause read issue with spark\n",
    "spark_final = spill.polars_to_spark(pl_processed, cleanup=False)\n",
    "\n",
    "print(\"ðŸš€ Converted back to Spark!\")\n",
    "spark_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb3bb2d-ede3-44a8-bc14-bb60360a77e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ’¾ Part 3: Checkpointing & Hybrid Storage Options\n",
    "Stop recalculating the same data. Save intermediate states to the Unity Catalog Volume (persistent) or the local driver disk (ephemeral/fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5482848-c801-4377-bc14-a8bc7778a4b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Save Polars DataFrame to Volume (Persistent)\n",
    "spill.save_checkpoint_pl(pl_processed, name=\"gold_users_vol\", storage=\"volume\")\n",
    "\n",
    "# 2. Save Polars DataFrame to Local Driver /tmp (Ephemeral, wiped on cluster restart)\n",
    "spill.save_checkpoint_pl(pl_processed, name=\"gold_users_local\", storage=\"local\")\n",
    "\n",
    "# 3. Save a PySpark DataFrame checkpoint\n",
    "spill.save_checkpoint_spark(spark_final, name=\"spark_checkpoint\")\n",
    "\n",
    "print(\"\\nðŸ“‚ Current Checkpoints:\")\n",
    "print(f\"  Volume Storage: {spill.list_checkpoints('volume')}\")\n",
    "print(f\"  Local Storage:  {spill.list_checkpoints('local')}\")\n",
    "\n",
    "# Reloading to verify it works\n",
    "reloaded_pl = spill.load_checkpoint_pl(\"gold_users_local\", storage=\"local\")\n",
    "print(f\"\\nâœ… Reloaded {reloaded_pl.height} rows from Local checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72aa9bfb-43a6-4e19-8218-a710bd64aba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§¹ Part 4: Cleanup\n",
    "The `teardown()` method behaves differently based on your `IS_DEV` flag.\n",
    "- **In Dev (`is_dev=True`)**: Leaves the Volume intact so you can inspect the Parquet files in the Catalog Explorer.\n",
    "- **In Prod (`is_dev=False`)**: Drops the volume completely to avoid clutter and storage costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcda966e-02ba-400c-97a7-b4a2bc67eeca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Cleanup mode: {'Preserving data for inspection' if IS_DEV else 'Dropping Volume completely'}\")\n",
    "spill.teardown()\n",
    "\n",
    "# Drop the temporary delta table we created for the comments showcase\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "print(\"Showcase complete! ðŸŽ‰\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "polars==1.38.1"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Showcase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
