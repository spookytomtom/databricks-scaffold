{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b440e3-a0c6-41fc-90b1-18509ef78b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b336dda-51f9-4bd7-8aa2-6f5268c3d17e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import poalrs\n",
    "import polars as pl\n",
    "# Import databricks essentials\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import uuid\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import functools\n",
    "import polars as pl\n",
    "from pyspark.sql import DataFrame as SparkDataFrame, SparkSession\n",
    "\n",
    "# Define project volume\n",
    "CATALOG_NAME = \"main\"\n",
    "SCHEMA_NAME = \"default\"\n",
    "VOLUME_NAME = \"my_test_volume\"\n",
    "\n",
    "spark = SparkSession.builder.appName(\"scaffold\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542ede5e-d165-4d43-86b4-eca85bc71878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Define class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6155e51c-ba32-41e3-ba84-58c5aad5b8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class VolumeSpiller:\n",
    "    def __init__(self, spark: SparkSession, catalog: str, schema: str, volume_name: str, is_dev: bool = False):\n",
    "        self.spark = spark\n",
    "        self.is_dev = is_dev\n",
    "        self.full_name = f\"{catalog}.{schema}.{volume_name}\"\n",
    "        self.volume_root = f\"/Volumes/{catalog}/{schema}/{volume_name}\"\n",
    "        \n",
    "        if is_dev:\n",
    "            # In Dev, we just want to make sure it exists so we don't lose data\n",
    "            self.spark.sql(f\"CREATE VOLUME IF NOT EXISTS {self.full_name}\")\n",
    "        else:\n",
    "            # In Prod, we want a clean slate, so we drop then create\n",
    "            self.spark.sql(f\"DROP VOLUME IF EXISTS {self.full_name}\")\n",
    "            self.spark.sql(f\"CREATE VOLUME {self.full_name}\")\n",
    "            \n",
    "        self._active_temp_dirs = []\n",
    "\n",
    "    def get_path(self, name: str) -> str:\n",
    "        \"\"\"Returns the absolute path for a named folder in the volume.\"\"\"\n",
    "        return f\"{self.volume_root}/{name.lstrip('/')}\"\n",
    "\n",
    "    def save_checkpoint(self, df: pl.DataFrame | pl.LazyFrame, name: str):\n",
    "        \"\"\"\n",
    "        Saves a Polars DataFrame/LazyFrame as a named checkpoint (directory).\n",
    "        Automates folder creation and file naming.\n",
    "        \"\"\"\n",
    "        checkpoint_dir = self.get_path(name)\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "        \n",
    "        # Standardizing on data.parquet inside the folder\n",
    "        target_path = f\"{checkpoint_dir}/data.parquet\"\n",
    "        \n",
    "        if isinstance(df, pl.LazyFrame):\n",
    "            df.sink_parquet(target_path, compression=\"zstd\")\n",
    "        else:\n",
    "            df.write_parquet(target_path, compression=\"zstd\")\n",
    "        \n",
    "        print(f\"âœ… Checkpoint '{name}' saved to UC Volume.\")\n",
    "\n",
    "    def load_checkpoint(self, name: str, eager: bool = True) -> pl.DataFrame | pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Loads a named checkpoint into Polars.\n",
    "        Uses glob patterns to stay immune to Spark/Polars metadata differences.\n",
    "        \"\"\"\n",
    "        checkpoint_dir = self.get_path(name)\n",
    "        glob_path = f\"{checkpoint_dir}/*.parquet\"\n",
    "        \n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            raise FileNotFoundError(f\"Checkpoint '{name}' not found at {checkpoint_dir}\")\n",
    "\n",
    "        return pl.read_parquet(glob_path) if eager else pl.scan_parquet(glob_path)\n",
    "\n",
    "    def teardown(self):\n",
    "        \"\"\"Cleans up. In Dev, we keep the data for inspection.\"\"\"\n",
    "        if self.is_dev:\n",
    "            print(f\"ðŸ› ï¸ DEV MODE: Data preserved at {self.volume_root}\")\n",
    "        else:\n",
    "            self.spark.sql(f\"DROP VOLUME IF EXISTS {self.full_name}\")\n",
    "            print(\"ðŸ—‘ï¸ PROD MODE: Volume dropped.\")\n",
    "\n",
    "    def spark_to_polars(self, df: SparkDataFrame, cleanup: bool = True, eager: bool = True, optimize_files: bool = True) -> pl.DataFrame | pl.LazyFrame:\n",
    "        run_id = uuid.uuid4().hex\n",
    "        temp_dir = self.get_path(f\"spill_{run_id}\")\n",
    "        \n",
    "        if not (cleanup and eager):\n",
    "            self._active_temp_dirs.append(temp_dir)\n",
    "\n",
    "        try:\n",
    "            if optimize_files:\n",
    "                df = df.coalesce(4) \n",
    "\n",
    "            # Spark writes the directory\n",
    "            df.write.mode(\"overwrite\").option(\"compression\", \"zstd\").parquet(temp_dir)\n",
    "\n",
    "            # FIX: Use a glob pattern to only read the .parquet files, ignoring Spark metadata\n",
    "            glob_path = f\"{temp_dir}/*.parquet\"\n",
    "            \n",
    "            if eager:\n",
    "                return pl.read_parquet(glob_path)\n",
    "            else:\n",
    "                return pl.scan_parquet(glob_path)\n",
    "\n",
    "        finally:\n",
    "            if cleanup and eager:\n",
    "                shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "    def polars_to_spark(self, df: pl.DataFrame | pl.LazyFrame, cleanup: bool = True) -> SparkDataFrame:\n",
    "        run_id = uuid.uuid4().hex\n",
    "        temp_dir = self.get_path(f\"spill_rev_{run_id}\")\n",
    "        \n",
    "        if not cleanup: \n",
    "             self._active_temp_dirs.append(temp_dir)\n",
    "\n",
    "        try:\n",
    "            # Ensure the directory exists\n",
    "            os.makedirs(temp_dir, exist_ok=True)\n",
    "            # We name the file 'part-0.parquet' inside the folder to mimic Spark behavior\n",
    "            file_path = f\"{temp_dir}/part-0.parquet\"\n",
    "            \n",
    "            if isinstance(df, pl.LazyFrame):\n",
    "                df.sink_parquet(file_path, compression=\"zstd\")\n",
    "            else:\n",
    "                df.write_parquet(file_path, compression=\"zstd\")\n",
    "\n",
    "            # Spark reads the directory, not the file\n",
    "            spark_df = self.spark.read.parquet(temp_dir)\n",
    "            \n",
    "            if cleanup:\n",
    "                spark_df.cache().count() \n",
    "                \n",
    "            return spark_df\n",
    "\n",
    "        finally:\n",
    "            if cleanup:\n",
    "                shutil.rmtree(temp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58db87b3-4772-4041-aa6d-4c84974c8b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Use class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9f790a9-3771-4838-997e-93f89832b32e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "IS_DEV = False  # Toggle this to False for production runs\n",
    "CATALOG = \"main\"\n",
    "SCHEMA = \"default\"\n",
    "VOL_NAME = \"my_pipeline_dev\" if IS_DEV else f\"vol_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "spiller = VolumeSpiller(spark, CATALOG, SCHEMA, VOL_NAME, is_dev=IS_DEV)\n",
    "\n",
    "try:\n",
    "    SCHEMA_NAME = T.StructType([\n",
    "        T.StructField(\"transaction_id\", T.IntegerType(), False),\n",
    "        T.StructField(\"customer_name\", T.StringType(), True),\n",
    "        T.StructField(\"product_category\", T.StringType(), True),\n",
    "        T.StructField(\"amount\", T.DoubleType(), True),\n",
    "        T.StructField(\"timestamp\", T.TimestampType(), True)\n",
    "    ])\n",
    "\n",
    "    data = []\n",
    "    categories = [\"Electronics\", \"Home\", \"Garden\", \"Books\", \"Toys\"]\n",
    "    names = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Edward\"]\n",
    "\n",
    "    for i in range(1, 21):\n",
    "        data.append((\n",
    "            i, \n",
    "            names[i % 5], \n",
    "            categories[i % 5], \n",
    "            round(10.5 * i, 2), \n",
    "            datetime(2024, 1, 1) + timedelta(days=i)\n",
    "        ))\n",
    "\n",
    "    my_spark_df = spark.createDataFrame(data, SCHEMA_NAME)\n",
    "    \n",
    "    # If we are in dev, maybe we don't clean up so we can check the file\n",
    "    pl_df = spiller.spark_to_polars(my_spark_df, cleanup=(not IS_DEV))\n",
    "    \n",
    "    spiller.save_checkpoint(pl_df, \"cleaned_data\")\n",
    "\n",
    "finally:\n",
    "    # This will only actually delete if IS_DEV is False\n",
    "    spiller.teardown()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "polars==1.38.1"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6822661054505834,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Example_usage",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
